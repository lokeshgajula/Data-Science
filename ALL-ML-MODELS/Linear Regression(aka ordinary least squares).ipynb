{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3022a0",
   "metadata": {},
   "source": [
    "# Linear Regression (aka ordinary least squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9884fe",
   "metadata": {},
   "source": [
    "Linear regression, or ordinary least square(OLS), is the simplest and most classic linear method for regression. Linear regression finds the parameters w and b that minimize the mean squared error between predictions and the true regression targets, y , on the training set. The mean squared error is the sum of the squared differences between the predictions and the true values, divided by the number of samples. Linear regression has no parameters, which is a benefit, but it also has no way to control model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22334c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X,y = mglearn.datasets.make_wave(n_samples=60)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8088ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e412e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr.coef_: [0.39390555]\n",
      "lr.intercept_:-0.031804343026759746\n"
     ]
    }
   ],
   "source": [
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_:{}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210147ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.67\n",
      "Test set score: 0.66\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score : {:.2f}\".format(lr.score(X_train,y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6cb88d",
   "metadata": {},
   "source": [
    "An R^2 of around 0.66 is not very good, but we can see that the scores on the training and test sets are very close together. This means we are likely underfitting, not overfitting. Fot this one-dimensional dataset, there is little danger of overfitting, as the model is very simple(or restricted). However, with high dimentional datasets(datasets with large features) , linear model becomes more powerful, and there is a high chance of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8752d48",
   "metadata": {},
   "source": [
    "# Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9361c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = mglearn.datasets.load_extended_boston()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8721b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042ac651",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4cb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.95\n",
      "Test set score : 0.61\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score : {:.2f}\".format(lr2.score(X_train,y_train)))\n",
    "print(\"Test set score : {:.2f}\".format(lr2.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09dd05",
   "metadata": {},
   "source": [
    "R^2 on test set is much worse. This is a clear sign of overfitting. So, we should find a model that allows us to control complexity. Ex: ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe0780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
