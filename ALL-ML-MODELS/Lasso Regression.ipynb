{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26572b3e",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0123ee",
   "metadata": {},
   "source": [
    "An alternative to Ridge for regularizing linear regression is Lasso. As with ridge regression, using the lasso also restricts coefficients to be close to zero, but in a slightly different way, called L1 regualarization. The consequence of L1 regularization is that when using the lasso, some coefficients are exactly zero. This means some features are entirely ignored by the model. This can be seen as a form of automatic feature selection. Having some coefficients be exactly zero often makes a model easier to interpret, and can reveal the most important features of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2b60c",
   "metadata": {},
   "source": [
    "# Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d07a2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import mglearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y = mglearn.datasets.load_extended_boston()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1ddd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.29\n",
      "Test set score: 0.21\n",
      "Number of features used: 4\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso().fit(X_train,y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train,y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test,y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ !=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88edcd52",
   "metadata": {},
   "source": [
    "As you can see, Lasso does quite badly, both on the training and the test set. This indicates that we are underfitting, and we find that it used only 4 of the 105 features. Similarly to Ridge, the Lasso also has a regularization parameter alpha. To reduce underfitting ,let's try decreasing alpha. When we do this, we need to increase the default settings of max_iter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad978c8",
   "metadata": {},
   "source": [
    "## alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6897c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.90\n",
      "Test set score : 0.77\n",
      "Number of features used : 33\n"
     ]
    }
   ],
   "source": [
    "lasso001 = Lasso(alpha = 0.01, max_iter=100000).fit(X_train,y_train)\n",
    "print(\"Training set score : {:.2f}\".format(lasso001.score(X_train,y_train)))\n",
    "print(\"Test set score : {:.2f}\".format(lasso001.score(X_test,y_test)))\n",
    "print(\"Number of features used : {}\".format(np.sum(lasso001.coef_!=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0efc40",
   "metadata": {},
   "source": [
    "## alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4747e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.95\n",
      "Test set score : 0.64\n",
      "Number of features used : 96\n"
     ]
    }
   ],
   "source": [
    "lasso00001 = Lasso(alpha = 0.0001, max_iter=100000).fit(X_train,y_train)\n",
    "print(\"Training set score : {:.2f}\".format(lasso00001.score(X_train,y_train)))\n",
    "print(\"Test set score : {:.2f}\".format(lasso00001.score(X_test,y_test)))\n",
    "print(\"Number of features used : {}\".format(np.sum(lasso00001.coef_!=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d776a7",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a108c7a0",
   "metadata": {},
   "source": [
    "In practice, ridge regression is usually the first choice between these two models. However, if you have a large amount of features and expect only a few of them to be important, Lasso might be a better choice. Similarly, if you would like to have a model that is easy to interpret, Lasso will provide a model that is easier to understand, as it will select only a subset of the input features. scikit-learn provides the ElasticNet class, which combines the penalties of Lasso and Ridge. In Practice, this combination works best, though at the price of having two parameters to adjust: one for L1 regularization, and one for L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c360f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
